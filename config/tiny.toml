[training]
batch_size = 64

[model]
n_layer = 4
n_embd = 128
n_head = 4
mlp_internal_dim_multiplier = 4
dropout = 0.1
