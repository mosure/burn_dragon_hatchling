# dataset type presets:
#   "shakespeare"  - tinyshakespeare character corpus
#   "deep_math"    - zwhe99/DeepMath-103K (Problem/Solution pairs)
#   "tiny_chat"    - starhopp3r/TinyChat BASIC-English conversations
#   "webscale_rl"  - Salesforce/Webscale-RL parquet shards (RL QA data)
#   "hugging_face" - arbitrary Hugging Face dataset (see example below)

[dataset]
cache_dir = "data/shakespeare"
train_split_ratio = 0.9
# type options listed above
type = "shakespeare"

# example for a custom huggingface dataset override:
#
# [dataset]
# cache_dir = "data"
# train_split_ratio = 0.85
# type = "hugging_face"
# format = "parquet"        # or "jsonl" / "text"
# repo_id = "zwhe99/DeepMath-103K"
# revision = "main"         # optional ref (tag/branch/sha)
# train_files = ["data/train.parquet"]
# validation_files = ["data/validation.parquet"]
# text_fields = ["text"]
# max_records = 1024        # optional sampling cap for prototyping
# authentication follows HF defaults (HF_TOKEN or huggingface-cli login)

[dataset.tokenizer]
# type accepts "char" (default) or "byte"
type = "char"
include_unknown = true
vocab_path = "vocab.json"

[training]
block_size = 64
batch_size = 16
max_iters = 100000
log_frequency = 100
# fraction of batch slots that keep streaming model state between steps (0.0 disables)
stream_retain_pct = 0.2
# optional cap on tokens kept in a stream's cached state (0 or unset = follow context_strategy)
# stream_max_context = 0

[optimizer]
learning_rate = 0.001
weight_decay = 0.1

[optimizer.lr_schedule]
# type variants: "constant", "cosine", "linear", "exponential", "step", "noam"
type = "cosine"
min_lr = 0.00005
num_iters = 100000

[generation]
prompt = "To be or "
max_tokens = 2048
temperature = 1.0
top_k = 3

[model]
n_layer = 6
n_embd = 256
n_head = 4
mlp_internal_dim_multiplier = 64
dropout = 0.1
fused_kernels = true
use_alibi = true
