[training]
batch_size = 4

[model]
n_layer = 8
n_embd = 512
n_head = 8
mlp_internal_dim_multiplier = 4
dropout = 0.1
